{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import os\n",
    "import csv\n",
    "import country_converter as coco\n",
    "import re\n",
    "from pyproj import Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the filename\n",
    "# File is in same directory as the code, so path of file doesn't need to be specified\n",
    "filename = 'links.csv'\n",
    "file_path = filename\n",
    "\n",
    "\n",
    "print(file_path)\n",
    "\n",
    "\n",
    "new_file = 'filtered_links.csv'\n",
    "new_file_path = new_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Filtered buses file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get filtered file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Coordinates: (22.939453, 40.663973)\n",
      "Destination Coordinates: (23.192139, 40.722283)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_coordinates(line_string):\n",
    "    matches = re.findall(r'\\(([^)]+)\\)', line_string)\n",
    "    if matches:\n",
    "        # Split the first match by ',' to separate the source and destination coordinates\n",
    "        src, dst = matches[0].split(',')\n",
    "        \n",
    "        # Split each coordinate by space to separate X and Y components and convert them to float\n",
    "        src_x, src_y = tuple(map(float, src.strip().split(' ')))\n",
    "        dst_x, dst_y = tuple(map(float, dst.strip().split(' ')))\n",
    "        \n",
    "        # Return the coordinates as tuples of floats\n",
    "        return (src_x, src_y), (dst_x, dst_y)\n",
    "    return (0.0, 0.0), (0.0, 0.0)  # Return zero coordinates if no matches are found\n",
    "\n",
    "# Testing the function\n",
    "src_coords, dst_coords = extract_coordinates('LINESTRING(22.939453 40.663973,23.192139 40.722283)')\n",
    "print(f\"Source Coordinates: {src_coords}\")\n",
    "print(f\"Destination Coordinates: {dst_coords}\")\n",
    "\n",
    "\n",
    "def extract_tag_values(tags_string, tag_name):\n",
    "    \n",
    "    # tags are separated by a space\n",
    "    tags_list = tags_string.split(\" \")\n",
    "    \n",
    "    for tag in tags_list:\n",
    "        tag = tag.strip(\"'\")\n",
    "        if \"=>\" in tag:\n",
    "            tag_name_info, value = tag.split(\"=>\")      #tag name and value separated by =>\n",
    "        \n",
    "            # goes through tags to see find specified tag name and info\n",
    "            if tag_name_info.strip(\"'\").strip() == tag_name:\n",
    "                return value.strip(\"'\").strip()         # additional .strip removes any extra spaces\n",
    "        \n",
    "    # Returns None if the specified tag name is not found\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', newline='') as csv_file, open(new_file_path, 'w', newline='') as new_csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    writer = csv.writer(new_csv_file)\n",
    "    \n",
    "    \n",
    "    #Writing header to new file\n",
    "    original_header = next(reader)\n",
    "    geometry_index = original_header.index('geometry')\n",
    "    new_header = original_header[:geometry_index] + ['src_coord', 'dst_coord', 'src_country', 'dst_country'] + original_header[geometry_index+1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    under_construction_index = new_header.index('under_construction')\n",
    "    tags_index = new_header.index('tags')\n",
    "    src_country_index = new_header.index('src_country')\n",
    "    dst_country_index = new_header.index('dst_country')\n",
    "    \n",
    "    writer.writerow(new_header)\n",
    "    \n",
    "    \n",
    "    for row in reader:\n",
    "        under_construction_value = row[under_construction_index]\n",
    "        \n",
    "        # Rows where the buses are underconstruction are not included\n",
    "        if under_construction_value != 't':\n",
    "            \n",
    "            \n",
    "            ## Adjusting tags string\n",
    "            # Combining the columns of tags in each row to be in one single column\n",
    "            # Last two columns in rows contain details about coordinates, not tags\n",
    "            last_tag_index = len(row) - 3\n",
    "            \n",
    "            tags_string = ' '.join(row[tags_index : last_tag_index])\n",
    "            \n",
    "            # manipulating the output of string\n",
    "            tags_string = tags_string.replace('\"', \"'\")\n",
    "            if tags_string.startswith(\"''\"):\n",
    "                tags_string = tags_string[1:]\n",
    "            if tags_string.endswith(\"''\"):\n",
    "                tags_string = tags_string[:-1]\n",
    "                \n",
    "            # Entire tags string is placed in single tags columns\n",
    "            row[tags_index] = tags_string\n",
    "            \n",
    "            # Duplicate information is deleted\n",
    "            row[tags_index + 1:-2] = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Adjusting the coordinates string\n",
    "            # coordinates are split across two columns in a row\n",
    "            combined_coords = f\"{row[-2]},{row[-1]}\"\n",
    "            \n",
    "            # Last two columns are replaced with the combined coordinates string\n",
    "            row = row[:-2] + [combined_coords]\n",
    "            \n",
    "            \n",
    "            # Extract source and destination coordinates from last columns in row\n",
    "            src_coord, dst_coord = extract_coordinates(row[-1])\n",
    "            \n",
    "            # Last column in row that contained the string for coordinates is now replaced by 2 columns of source and destination coordinates\n",
    "            row = row[:-1] + [src_coord, dst_coord]\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Extracting particular tag information\n",
    "            \n",
    "            # Access source and destination country and create new column\n",
    "            src_country = extract_tag_values(tags_string, \"country_1\")\n",
    "            row.append(src_country)\n",
    "            \n",
    "            dst_country = extract_tag_values(tags_string, \"country_2\")\n",
    "            row.append(dst_country)\n",
    "            \n",
    "            if row[src_country_index] != 'NaN' or row[dst_country_index] != 'Nan':\n",
    "                writer.writerow(row)\n",
    "            \n",
    "            #writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8889, 14)\n",
      "\n",
      "Number of NaN values in 'src_country' column: 1345\n",
      "\n",
      "Number of NaN values in 'dst_country' column: 1407\n",
      "Number of occurrences of each country code1:\n",
      "src_country\n",
      "NaN    1345\n",
      "FR      905\n",
      "RU      859\n",
      "ES      689\n",
      "DE      545\n",
      "IT      369\n",
      "GB      320\n",
      "UA      229\n",
      "SE      220\n",
      "PL      204\n",
      "EG      196\n",
      "NO      188\n",
      "DK      187\n",
      "LY      185\n",
      "MA      180\n",
      "CH      176\n",
      "TR      166\n",
      "SY      151\n",
      "DZ      149\n",
      "RO      129\n",
      "PT      119\n",
      "TN      102\n",
      "CZ       95\n",
      "FI       92\n",
      "BY       76\n",
      "BG       72\n",
      "AT       70\n",
      "BA       67\n",
      "JO       63\n",
      "RS       60\n",
      "BE       58\n",
      "SK       54\n",
      "HU       52\n",
      "IE       52\n",
      "IS       39\n",
      "HR       36\n",
      "NL       35\n",
      "GR       33\n",
      "LT       29\n",
      "IL       28\n",
      "AL       28\n",
      "KZ       28\n",
      "MD       26\n",
      "SA       25\n",
      "EE       24\n",
      "LB       19\n",
      "IQ       18\n",
      "LV       17\n",
      "CY       13\n",
      "LU       11\n",
      "ME       11\n",
      "MK       11\n",
      "SI       11\n",
      "NI        9\n",
      "GE        8\n",
      "IR        3\n",
      "AZ        2\n",
      "AM        1\n",
      "Name: count, dtype: int64\n",
      "Number of occurrences of each country code2:\n",
      "dst_country\n",
      "NaN    1407\n",
      "FR      910\n",
      "RU      880\n",
      "ES      670\n",
      "DE      525\n",
      "IT      382\n",
      "GB      318\n",
      "UA      277\n",
      "SE      240\n",
      "PL      204\n",
      "EG      193\n",
      "MA      184\n",
      "LY      184\n",
      "DK      183\n",
      "NO      182\n",
      "TR      177\n",
      "SY      154\n",
      "CH      154\n",
      "DZ      144\n",
      "RO      130\n",
      "PT      128\n",
      "TN      106\n",
      "RS       89\n",
      "CZ       87\n",
      "FI       85\n",
      "AT       65\n",
      "JO       62\n",
      "SK       57\n",
      "BG       53\n",
      "HU       50\n",
      "BY       50\n",
      "BE       45\n",
      "NL       44\n",
      "HR       42\n",
      "IE       42\n",
      "IS       39\n",
      "BA       38\n",
      "GR       32\n",
      "LT       29\n",
      "SA       25\n",
      "LV       23\n",
      "IL       23\n",
      "AL       22\n",
      "SI       20\n",
      "LB       18\n",
      "LU       17\n",
      "IQ       16\n",
      "EE       15\n",
      "ME       14\n",
      "NI       14\n",
      "MK       13\n",
      "CY       13\n",
      "PA        6\n",
      "MD        4\n",
      "GE        3\n",
      "KZ        1\n",
      "MT        1\n",
      "Name: count, dtype: int64\n",
      "IQ: Iraq\n",
      "TR: TÃ¼rkiye\n",
      "NO: Norway\n",
      "CH: Switzerland\n",
      "SY: Syria\n",
      "DK: Denmark\n",
      "JO: Jordan\n",
      "IR: Iran\n",
      "IL: Israel\n",
      "EG: Egypt\n",
      "BE: Belgium\n",
      "GR: Greece\n",
      "HU: Hungary\n",
      "SA: Saudi Arabia\n",
      "GE: Georgia\n",
      "DE: Germany\n",
      "FR: France\n",
      "MK: North Macedonia\n",
      "SK: Slovakia\n",
      "IE: Ireland\n",
      "SE: Sweden\n",
      "TN: Tunisia\n",
      "CZ: Czechia\n",
      "IT: Italy\n",
      "AM: Armenia\n",
      "CY: Cyprus\n",
      "PA: Panama\n",
      "MA: Morocco\n",
      "LV: Latvia\n",
      "KZ: Kazakhstan\n",
      "ES: Spain\n",
      "SI: Slovenia\n",
      "LY: Libya\n",
      "RO: Romania\n",
      "RS: Serbia\n",
      "EE: Estonia\n",
      "RU: Russia\n",
      "PL: Poland\n",
      "BG: Bulgaria\n",
      "MT: Malta\n",
      "BA: Bosnia and Herzegovina\n",
      "LU: Luxembourg\n",
      "MD: Moldova\n",
      "AL: Albania\n",
      "DZ: Algeria\n",
      "NI: Nicaragua\n",
      "FI: Finland\n",
      "ME: Montenegro\n",
      "IS: Iceland\n",
      "UA: Ukraine\n",
      "GB: United Kingdom\n",
      "BY: Belarus\n",
      "NL: Netherlands\n",
      "LT: Lithuania\n",
      "AZ: Azerbaijan\n",
      "LB: Lebanon\n",
      "PT: Portugal\n",
      "AT: Austria\n",
      "HR: Croatia\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(new_file_path)\n",
    "print(df.shape)\n",
    "## 8889 14\n",
    "\n",
    "nan_count1 = df['src_country'].isna().sum()\n",
    "nan_count2 = df['dst_country'].isna().sum()\n",
    "\n",
    "country_code_counts1 = df['src_country'].value_counts(dropna=False)\n",
    "country_code_counts2 = df['dst_country'].value_counts(dropna=False)\n",
    "\n",
    "print(\"\\nNumber of NaN values in 'src_country' column:\", nan_count1)\n",
    "print(\"\\nNumber of NaN values in 'dst_country' column:\", nan_count2)\n",
    "\n",
    "print(\"Number of occurrences of each country code1:\")\n",
    "print(country_code_counts1)\n",
    "print(\"Number of occurrences of each country code2:\")\n",
    "print(country_code_counts2)\n",
    "\n",
    "#nan_count 3\n",
    "\n",
    "\n",
    "# Create a set of unique country codes from both 'src_country' and 'dst_country' columns excluding NaN\n",
    "unique_country_codes = set(df['src_country'].dropna()).union(set(df['dst_country'].dropna()))\n",
    "\n",
    "# Initialize a dictionary to store conversion results\n",
    "conversion_results = {}\n",
    "\n",
    "# Iterate over each unique country code\n",
    "for code in unique_country_codes:\n",
    "    # Convert the country code to the corresponding country name\n",
    "    country_name = coco.convert(names=code, to='name_short')\n",
    "    \n",
    "    # Store the conversion result in the dictionary\n",
    "    conversion_results[code] = country_name\n",
    "\n",
    "# Print the conversion results\n",
    "for code, name in conversion_results.items():\n",
    "    print(f\"{code}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Country Specified dataframes and csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: IE.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to create country csv files\n",
    "\n",
    "\n",
    "def get_country_network_data(file_path, country_id, voltages):\n",
    "    # Load the DataFrame\n",
    "    EU_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter based on country and voltage\n",
    "    filtered_df = EU_df[((EU_df['src_country'] == country_id) | (EU_df['dst_country'] == country_id)) & EU_df['voltage'].isin(voltages)].copy()\n",
    "    filtered_df.sort_values(by = 'voltage' , ascending = True, inplace = True)\n",
    "\n",
    "    # Function to decide which bus ID to keep\n",
    "    def decide_bus_id(existing_id, new_id):\n",
    "        # Keeps the smaller bus ID\n",
    "        return min(existing_id, new_id)\n",
    "\n",
    "    # Dealing with duplicate nodes of the same coordinates but different bus IDs\n",
    "    coord_to_bus_id = {}\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        for coord_type in ['src_coord', 'dst_coord']:\n",
    "            coord = row[coord_type]\n",
    "            bus_id = row[coord_type.split('_')[0] + '_bus_id']\n",
    "            \n",
    "            if coord not in coord_to_bus_id:\n",
    "                coord_to_bus_id[coord] = bus_id\n",
    "            else:\n",
    "                coord_to_bus_id[coord] = decide_bus_id(coord_to_bus_id[coord], bus_id)\n",
    "\n",
    "    # Update the DataFrame using the mappings\n",
    "    filtered_df.loc[:, 'src_bus_id'] = filtered_df['src_coord'].map(coord_to_bus_id)\n",
    "    filtered_df.loc[:, 'dst_bus_id'] = filtered_df['dst_coord'].map(coord_to_bus_id)\n",
    "\n",
    "    # Coordinate transformation\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "    def transform_coord(coord):\n",
    "        if isinstance(coord, str):\n",
    "            coord = coord.strip(\"()\").split(\",\")\n",
    "            coord = tuple(map(float, coord))\n",
    "        return transformer.transform(*coord)\n",
    "\n",
    "    # Apply the coordinate transformation\n",
    "    filtered_df[['src_x', 'src_y']] = filtered_df['src_coord'].apply(transform_coord).apply(pd.Series)\n",
    "    filtered_df[['dst_x', 'dst_y']] = filtered_df['dst_coord'].apply(transform_coord).apply(pd.Series)\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file named after the specified country\n",
    "    output_file_path = f\"{country_id}.csv\"\n",
    "    filtered_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"File saved: {output_file_path}\")\n",
    "\n",
    "\"\"\" # Example usage of the function\n",
    "\n",
    "new_file_path = 'path_to_your_data_file.csv'\n",
    "process_electric_network_data(new_file_path, 'IE', [220, 380]) \"\"\"\n",
    "\n",
    "\n",
    "EU_filtered_data = new_file_path\n",
    "get_country_network_data(EU_filtered_data, 'IE', [220, 380])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ireland Dataframe for 220kV and 380kV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: IE.csv\n"
     ]
    }
   ],
   "source": [
    "EU_filtered_data = new_file_path\n",
    "get_country_network_data(EU_filtered_data, 'IE', [220, 380])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
